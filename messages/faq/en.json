{
  "faq": {
    "title": "FAQ",
    "q1": "What is this?",
    "a1": "This is a demo of a Russian-Circassian translator app. It is powered by a machine learning model trained on Russian-Circassian sentence pairs and can also perform translations from over 100 other languages, though accuracy may vary. The purpose of this demo is to show that recent advances in machine learning have made it possible to create a translator for the Circassian language and to invite those interested to learn more about contributing to its development.",
    "q2": "The translations are wrong!",
    "a2": "This is just a demo version of the translator, not the final product. The model it runs on has been trained on a relatively small number of sentence pairs (~44K), whereas an optimal minimum is usually considered to be in the hundreds of thousands or more. If you would like to help improve the model's performance, you can <a href=\"#how-can-i-help\" class=\"underline hover:text-gray-600 dark:text-gray-200 dark:hover:text-gray-400\">join the effort to gather material</a> to improve it.",
    "q3": "Why is it only for Kabardian dialect?",
    "a3": "The language model was trained on Russian-Kabardian sentence pairs. However, it could easily be adapted to translate to other Circassian dialects in the future with enough additional training data. The main challenge is collecting enough text in these dialects to train the model effectively. If you're interested in helping to collect more text and improve the model's performance, you're welcome to <a href=\"#how-can-i-help\">contribute</a>.",
    "q4": "Techincal details",
    "a4": "This demo uses a fine-tuned version of the <a href=\"https://huggingface.co/facebook/m2m100_418M\" class=\"underline hover:text-gray-600 dark:text-gray-200 dark:hover:text-gray-400\">facebook/m2m100_418M</a> model. The model was fine-tuned on the \"ru-kbd\" dataset, which consists of ~44K sentences from books, textbooks, dictionaries, etc. The fine-tuned model achieved a Bleu score of 22.389 on the evaluation set. You can find more information about the model and the dataset at the following links:<br><br>Base m2m100_418M model: <a href=\"https://huggingface.co/facebook/m2m100_418M\" class=\"underline hover:text-gray-600 dark:text-gray-200 dark:hover:text-gray-400\">https://huggingface.co/facebook/m2m100_418M</a><br>ru-kbd model: <a href=\"https://huggingface.co/anzorq/m2m100_418M_ft_ru-kbd_44K\" class=\"underline hover:text-gray-600 dark:text-gray-200 dark:hover:text-gray-400\">https://huggingface.co/anzorq/m2m100_418M_ft_ru-kbd_44K</a><br>Paper: <a href=\"https://arxiv.org/abs/2010.11125\" class=\"underline hover:text-gray-600 dark:text-gray-200 dark:hover:text-gray-400\">https://arxiv.org/abs/2010.11125</a><br>Dataset: <a href=\"https://huggingface.co/datasets/anzorq/kbd-ru\" class=\"underline hover:text-gray-600 dark:text-gray-200 dark:hover:text-gray-400\">https://huggingface.co/datasets/anzorq/kbd-ru</a>",
    "q5": "<a href=\"#how-can-i-help\" id=\"how-can-i-help\" style=\"cursor: default;\">How can I help?</a>",
    "a5": "The quality of translations is directly proportional to the amount of text on which the model is trained. The current version of the model was trained on a very minimal number of sentence pairs, ~44K. In order to achieve good translation results, much more data is needed. You can help improve translation accuracy by contributing to collecting more data for training. For example, a monolingual text in any Circassian dialect or a bilingual text such as a book written in Circassian and translated into another language or vice versa, etc. The text can be in plain text, PDF or links to web pages containing the text. You can also help with scanning books in Circassian and/or converting scanned documents into text (OCR). If you are interested in participating in this project, join our Discord server: <a href=\"https://discord.gg/ppmwTNUZQb\" class=\"underline hover:text-gray-600 dark:text-gray-200 dark:hover:text-gray-400\">https://discord.gg/ppmwTNUZQb</a>"
  }
}
